import os
import io
import time
import json
import tempfile
import requests
import numpy as np
import pandas as pd
import streamlit as st
import logging
from typing import Optional, Tuple, Dict, Any
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv
from gtts import gTTS
from st_audiorec import st_audiorec
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

# Langchain / Groq imports
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
# ------------------- Safe TTS Warm-up -------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ------------------- Load environment variables -------------------
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
WEATHER_API_KEY = os.getenv("WEATHER_API_KEY", "").strip()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

# Enhanced validation
if not GROQ_API_KEY:
    st.error("‚ùå .env ‡§´‡§º‡§æ‡§á‡§≤ ‡§Æ‡•á‡§Ç `GROQ_API_KEY` ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç ‚Äî ‡§Ø‡§π LLM ‡§î‡§∞ ‡§∏‡•ç‡§™‡•Ä‡§ö APIs ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§")
    st.info("üí° Groq API key ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è https://console.groq.com ‡§™‡§∞ ‡§ú‡§æ‡§è‡§Ç")
    st.stop()

if not OPENAI_API_KEY:
    logger.warning("‚ö†Ô∏è OPENAI_API_KEY not set. Voice responses will use gTTS fallback.")

# Initialize OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
class UnifiedTTSSystem:
    """Unified TTS system with OpenAI primary and gTTS fallback"""
    def __init__(self):
        self.openai_available = openai_client is not None
        self.gtts_available = True
        self.cache = {}  # Simple audio cache
        
    def generate_audio(self, text: str, use_cache: bool = True) -> Optional[bytes]:
        """Generate audio with primary/fallback logic"""
        if not text or not text.strip():
            return None
            
        # Check cache
        text_hash = hash(text[:500])  # Cache key
        if use_cache and text_hash in self.cache:
            logger.info("Using cached audio")
            return self.cache[text_hash]
        
        # Truncate long text
        if len(text) > 500:
            text = self._truncate_intelligently(text, 500)
        
        # Try OpenAI first
        if self.openai_available:
            audio_bytes = self._openai_tts(text)
            if audio_bytes:
                if use_cache and len(self.cache) < 20:
                    self.cache[text_hash] = audio_bytes
                return audio_bytes
            logger.warning("OpenAI TTS failed, falling back to gTTS")
        
        # Fallback to gTTS
        if self.gtts_available:
            audio_bytes = self._gtts_tts(text)
            if audio_bytes and use_cache and len(self.cache) < 20:
                self.cache[text_hash] = audio_bytes
            return audio_bytes
        
        return None
    
    def _openai_tts(self, text: str) -> Optional[bytes]:
        """OpenAI TTS implementation"""
        try:
            response = openai_client.audio.speech.create(
                model="tts-1",  # FIXED: correct model name
                voice="alloy",
                input=text[:4096]  # OpenAI limit
            )
            return response.read()
        except Exception as e:
            logger.error(f"OpenAI TTS failed: {e}")
            return None
    
    def _gtts_tts(self, text: str) -> Optional[bytes]:
        """gTTS fallback implementation"""
        try:
            tts = gTTS(text=text, lang="hi", slow=False)
            audio_buffer = io.BytesIO()
            tts.write_to_fp(audio_buffer)
            audio_buffer.seek(0)
            return audio_buffer.getvalue()
        except Exception as e:
            logger.error(f"gTTS failed: {e}")
            return None
    
    def _truncate_intelligently(self, text: str, max_length: int) -> str:
        """Truncate text at sentence boundaries"""
        if len(text) <= max_length:
            return text
        
        sentences = text.split('‡•§')
        truncated = ""
        for sentence in sentences:
            if len(truncated + sentence + "‡•§") <= max_length:
                truncated += sentence + "‡•§"
            else:
                break
        
        return truncated if truncated else text[:max_length] + "..."

# ------------------- Speech-to-Text -------------------
class SpeechToText:
    """Enhanced speech-to-text with error handling"""
    
    @staticmethod
    def transcribe(file_path: str, language: str = "hi") -> str:
        """Transcribe audio file using Groq Whisper API"""
        if not os.path.exists(file_path):
            logger.error(f"Audio file not found: {file_path}")
            return ""
        
        try:
            url = "https://api.groq.com/openai/v1/audio/transcriptions"
            headers = {"Authorization": f"Bearer {GROQ_API_KEY}"}
            
            with open(file_path, "rb") as audio_file:
                files = {"file": (os.path.basename(file_path), audio_file, "audio/wav")}
                data = {
                    "model": "whisper-large-v3",
                    "language": language,
                    "response_format": "text"
                }
                response = requests.post(
                    url, 
                    headers=headers, 
                    data=data, 
                    files=files, 
                    timeout=45
                )
            
            response.raise_for_status()
            return response.text.strip()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Transcription API error: {e}")
            return ""
        except Exception as e:
            logger.error(f"Unexpected transcription error: {e}")
            return ""


# ------------------- Page config & Enhanced CSS -------------------
st.set_page_config(
    page_title="üåæ AI ‡§ï‡•É‡§∑‡§ø ‡§∏‡§π‡§æ‡§Ø‡§ï", 
    layout="wide", 
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': "AI ‡§ï‡•É‡§∑‡§ø ‡§∏‡§π‡§æ‡§Ø‡§ï - ‡§Ü‡§™‡§ï‡§æ ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§ñ‡•á‡§§‡•Ä ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞"
    }
)

st.markdown("""
<style>
    .main-title { 
        text-align: center; 
        color: #2E8B57; 
        font-size: 2.2rem; 
        margin-bottom: 1rem; 
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .voice-section { 
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
        padding: 1.5rem; 
        border-radius: 12px; 
        color: white; 
        margin: 1rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .chat-container {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        border-left: 4px solid #28a745;
    }
    .user-message {
        background-color: #e3f2fd;
        padding: 0.8rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 3px solid #2196f3;
    }
    .assistant-message {
        background-color: #f1f8e9;
        padding: 0.8rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 3px solid #4caf50;
    }
    .metric-card {
        background-color: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
    }
    .status-success {
        background-color: #d4edda;
        color: #155724;
        padding: 0.5rem;
        border-radius: 4px;
        border: 1px solid #c3e6cb;
    }
    .status-error {
        background-color: #f8d7da;
        color: #721c24;
        padding: 0.5rem;
        border-radius: 4px;
        border: 1px solid #f5c6cb;
    }
    .status-info {
        background-color: #d1ecf1;
        color: #0c5460;
        padding: 0.5rem;
        border-radius: 4px;
        border: 1px solid #bee5eb;
    }
    .loading-spinner {
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 1rem;
    }
</style>
""", unsafe_allow_html=True)
# ------------------- Web Geolocation -------------------

def get_location_html():
    """Generate HTML for browser geolocation"""
    return """
    <div id="location-container">
        <button id="get-location-btn" onclick="getLocation()" 
                style="background: linear-gradient(45deg, #FF6B6B, #4ECDC4); 
                       color: white; padding: 10px 20px; border: none; 
                       border-radius: 20px; cursor: pointer;">
            üìç ‡§Ö‡§™‡§®‡§æ ‡§∏‡§ü‡•Ä‡§ï ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡•á‡§Ç
        </button>
        <div id="location-status" style="margin-top: 10px; font-size: 14px;"></div>
    </div>

    <script>
    function getLocation() {
        const statusDiv = document.getElementById('location-status');
        const btn = document.getElementById('get-location-btn');
        
        if (navigator.geolocation) {
            btn.innerHTML = '‚è≥ ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...';
            btn.disabled = true;
            
            navigator.geolocation.getCurrentPosition(
                function(position) {
                    const lat = position.coords.latitude;
                    const lng = position.coords.longitude;
                    const accuracy = position.coords.accuracy;
                    
                    statusDiv.innerHTML = `
                        ‚úÖ ‡§∏‡•ç‡§•‡§æ‡§® ‡§Æ‡§ø‡§≤ ‡§ó‡§Ø‡§æ!<br>
                        üìç ‡§Ö‡§ï‡•ç‡§∑‡§æ‡§Ç‡§∂: ${lat.toFixed(4)}<br>
                        üìç ‡§¶‡•á‡§∂‡§æ‡§Ç‡§§‡§∞: ${lng.toFixed(4)}<br>
                        üéØ ‡§∏‡§ü‡•Ä‡§ï‡§§‡§æ: ${Math.round(accuracy)}m
                    `;
                    
                    btn.innerHTML = '‚úÖ ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ';
                },
                function(error) {
                    let errorMsg = '';
                    switch(error.code) {
                        case error.PERMISSION_DENIED:
                            errorMsg = '‚ùå ‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä';
                            break;
                        case error.POSITION_UNAVAILABLE:
                            errorMsg = '‚ö†Ô∏è ‡§∏‡•ç‡§•‡§æ‡§® ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç';
                            break;
                        case error.TIMEOUT:
                            errorMsg = '‚è∞ ‡§∏‡§Æ‡§Ø ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ';
                            break;
                        default:
                            errorMsg = '‚ùå ‡§Ö‡§ú‡•ç‡§û‡§æ‡§§ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø';
                            break;
                    }
                    statusDiv.innerHTML = errorMsg + '<br><small>IP ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç</small>';
                    btn.innerHTML = 'üìç ‡§™‡•Å‡§®‡§É ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•á‡§Ç';
                    btn.disabled = false;
                }
            );
        } else {
            statusDiv.innerHTML = '‚ùå ‡§Ø‡§π ‡§¨‡•ç‡§∞‡§æ‡§â‡§ú‡§º‡§∞ geolocation ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ';
        }
    }
    </script>
    """











# ------------------- Session state initialization -------------------
def init_session_state():
    """Initialize all session state variables"""
    defaults = {
        "app_initialized": False,
        "tts_system_ready": False,
        "stt_warmed": False,
        "chat_history": [],
        "processing": False,
        "last_audio_data": None,
        "voice_enabled": True,
        "auto_play_response": True,
        "use_offline_tts": False, 
        "location_method": "ip",
        "html_location": None,
        "warmup_status": "‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...",
        "tts_system": UnifiedTTSSystem(),
        "stt": SpeechToText()
    }
    
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

init_session_state()

if "tts_system" not in st.session_state:
    st.session_state.tts_system = UnifiedTTSSystem()

st.markdown('<h1 class="main-title">üåæ AI ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§´‡§∏‡§≤ ‡§∏‡§≤‡§æ‡§π ‡§∏‡§π‡§æ‡§Ø‡§ï (‡§π‡§ø‡§Ç‡§¶‡•Ä, ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§∏‡§π‡§ø‡§§)</h1>', unsafe_allow_html=True)
# Enhanced initialization with better UX
def perform_comprehensive_warmup():
    """Perform comprehensive system warmup with progress tracking"""
    if st.session_state.app_initialized:
        return True
    
    progress_container = st.container()
    
    with progress_container:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        warmup_steps = [
            ("üîß ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠...", 15),
            ("üé§ ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...", 35),
            ("üîä TTS ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§µ‡§æ‡§∞‡•ç‡§Æ ‡§Ö‡§™...", 55),
            ("üåê API ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§ú‡§æ‡§Ç‡§ö...", 70),
            ("üìä ‡§°‡•á‡§ü‡§æ ‡§∏‡•ã‡§∞‡•ç‡§∏ ‡§ï‡§®‡•á‡§ï‡•ç‡§ü...", 85),
            ("‚úÖ ‡§∏‡§≠‡•Ä ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞!", 100)
        ]
        
        for step_text, progress_value in warmup_steps:
            status_text.markdown(f'<div class="warmup-status">{step_text}</div>', unsafe_allow_html=True)
            progress_bar.progress(progress_value)
            
            # Actual warmup operations
            if progress_value == 35:  # STT warmup simulation
                try:
                    test_response = requests.get(
                        "https://api.groq.com/openai/v1/models", 
                        headers={"Authorization": f"Bearer {GROQ_API_KEY}"}, 
                        timeout=5
                    )
                    if test_response.status_code == 200:
                        st.session_state.stt_warmed = True
                        st.session_state.warmup_status = "STT API ‡§§‡•à‡§Ø‡§æ‡§∞ ‚úÖ"
                    else:
                        st.session_state.warmup_status = "STT API ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‚ö†Ô∏è"
                except Exception as e:
                    st.session_state.stt_warmed = False
                    st.session_state.warmup_status = "STT API ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‚ö†Ô∏è"
                    logger.error(f"STT warmup failed: {e}")
            
            elif progress_value == 55:  # TTS warmup
                try:
                    tts_success = True
                    st.session_state.tts_system_ready = True

                    st.session_state.tts_system_ready = tts_success
                    if tts_success:
                        st.session_state.warmup_status = "TTS ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‚úÖ"
                    else:
                        st.session_state.warmup_status = "TTS ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‚ö†Ô∏è"
                except Exception as e:
                    logger.error(f"TTS warmup failed: {e}")
                    st.session_state.tts_system_ready = False
                    st.session_state.warmup_status = "TTS ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‚ùå"
            
            time.sleep(0.6)
        
        time.sleep(1)
        progress_container.empty()
    
    st.session_state.app_initialized = True
    return True
perform_comprehensive_warmup()

# ------------------- Enhanced utility functions -------------------
def get_default_soil_data() -> Dict[str, float]:
    """Return default soil data for fallback"""
    return {
        "ph": 6.5,
        "nitrogen": 50,
        "organic_carbon": 10,
        "sand": 40,
        "silt": 40,
        "clay": 20
    }

def get_default_weather_data() -> Dict[str, Any]:
    """Return default weather data for fallback"""
    return {
        "temperature": 25,
        "humidity": 70,
        "precipitation": 2,
        "wind_speed": 10,
        "condition": "‡§∏‡§æ‡§´‡§º"
    }

@st.cache_data(ttl=3600, show_spinner=False)
def get_user_location() -> Tuple[float, float, str]:
    """Get user location with HTML geolocation support"""
    # Check if HTML geolocation data is available
    if st.session_state.html_location:
        lat = st.session_state.html_location.get("lat", 28.61)
        lon = st.session_state.html_location.get("lng", 77.20)
        return lat, lon, "HTML Geolocation"
    
    # Fallback to IP-based location
    try:
        response = requests.get("https://ipinfo.io/json", timeout=8)
        if response.status_code == 200:
            data = response.json()
            loc = data.get("loc", "28.61,77.20").split(",")
            city = data.get("city", "‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä")
            region = data.get("region", "")
            
            location_name = f"{city}"
            if region and region != city:
                location_name += f", {region}"
                
            return float(loc[0]), float(loc[1]), location_name
    except Exception as e:
        logger.warning(f"Location fetch failed: {e}")
    
    return 28.61, 77.20, "‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä"  

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_soil(lat: float, lon: float) -> Dict[str, float]:
    """Fetch soil data with better error handling and realistic defaults"""
    try:
        # Using a more reliable approach for soil data
        url = "https://rest.isric.org/soilgrids/v2.0/properties"
        params = {
            "lon": lon,
            "lat": lat,
            "property": "phh2o",
            "depth": "0-5cm",
            "value": "mean"
        }
        
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            # For demo purposes, return realistic values based on location
            # In production, you would parse the actual SoilGrids response
            base_soil = get_default_soil_data()
            
            # Adjust values slightly based on location for realism
            lat_factor = (lat - 20) / 20  # Normalize around typical Indian latitudes
            
            base_soil["ph"] += lat_factor * 0.5
            base_soil["nitrogen"] += lat_factor * 10
            
            return base_soil
            
    except requests.RequestException as e:
        logger.warning(f"Soil data fetch failed: {e}")
    except Exception as e:
        logger.error(f"Unexpected error in soil data fetch: {e}")
    
    return get_default_soil_data()

@st.cache_data(ttl=600, show_spinner=False)  # 10 minute cache for weather
def fetch_weather(lat: float, lon: float) -> Dict[str, Any]:
    """Fetch weather data with comprehensive error handling"""
    if not WEATHER_API_KEY:
        return get_default_weather_data()
        
    try:
        url = "http://api.weatherapi.com/v1/current.json"
        params = {
            "key": WEATHER_API_KEY,
            "q": f"{lat},{lon}",
            "aqi": "no"
        }
        
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            current = data.get("current", {})
            
            return {
                "temperature": current.get("temp_c", 25),
                "humidity": current.get("humidity", 70),
                "precipitation": current.get("precip_mm", 2),
                "wind_speed": current.get("wind_kph", 10),
                "condition": current.get("condition", {}).get("text", "‡§∏‡§æ‡§´‡§º"),
                "feels_like": current.get("feelslike_c", 25),
                "uv": current.get("uv", 5)
            }
        else:
            logger.warning(f"Weather API returned status {response.status_code}")
            
    except requests.RequestException as e:
        logger.warning(f"Weather data fetch failed: {e}")
    except Exception as e:
        logger.error(f"Unexpected error in weather data fetch: {e}")
    
    return get_default_weather_data()

# ------------------- Enhanced ML model -------------------
@st.cache_resource(show_spinner=False)
def get_trained_model() -> Tuple[RandomForestClassifier, StandardScaler]:
    """Create and train enhanced ML model"""
    np.random.seed(42)
    n_samples = 2000  # More training data
    
    features = []
    labels = []
    
    # Generate more diverse and realistic training data
    for _ in range(n_samples):
        temp = np.random.normal(25, 10)
        humidity = np.random.normal(70, 20)
        ph = np.random.normal(6.5, 1.2)
        nitrogen = np.random.normal(50, 25)
        
        features.append([temp, humidity, ph, nitrogen])
        
        # Enhanced decision logic for crop recommendation
        if temp < 22 and humidity > 55 and ph > 6.0:
            labels.append(0)  # ‡§ó‡•á‡§π‡•Ç‡§Å
        elif temp > 28 and humidity > 75 and ph < 7.5:
            labels.append(1)  # ‡§ß‡§æ‡§®
        elif temp > 20 and temp < 35 and humidity < 80:
            labels.append(2)  # ‡§Æ‡§ï‡•ç‡§ï‡§æ
        else:
            # Random assignment for edge cases
            labels.append(np.random.choice([0, 1, 2]))
    
    X = np.array(features)
    y = np.array(labels)
    
    # Feature scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Train model with better parameters
    clf = RandomForestClassifier(
        n_estimators=150,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    clf.fit(X_scaled, y)
    
    return clf, scaler

def get_crop_prediction(soil: Dict[str, float], weather: Dict[str, Any]) -> Tuple[str, float]:
    """Get crop prediction with confidence score"""
    try:
        clf, scaler = get_trained_model()
        
        features = np.array([[
            weather.get("temperature", 25),
            weather.get("humidity", 70),
            soil.get("ph", 6.5),
            soil.get("nitrogen", 50)
        ]])
        
        features_scaled = scaler.transform(features)
        probabilities = clf.predict_proba(features_scaled)[0]
        prediction = int(clf.predict(features_scaled)[0])
        
        crop_map = {0: "üåæ ‡§ó‡•á‡§π‡•Ç‡§Å", 1: "üå± ‡§ß‡§æ‡§®", 2: "üåΩ ‡§Æ‡§ï‡•ç‡§ï‡§æ"}
        confidence = float(max(probabilities) * 100)
        
        return crop_map.get(prediction, "‚ùì ‡§Ö‡§ú‡•ç‡§û‡§æ‡§§"), confidence
        
    except Exception as e:
        logger.error(f"Crop prediction failed: {e}")
        return "üåæ ‡§ó‡•á‡§π‡•Ç‡§Å", 75.0

# ------------------- Load environmental data -------------------
with st.spinner("üåç ‡§∏‡•ç‡§•‡§æ‡§® ‡§î‡§∞ ‡§™‡§∞‡•ç‡§Ø‡§æ‡§µ‡§∞‡§£ ‡§°‡•á‡§ü‡§æ ‡§≤‡•ã‡§° ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç..."):
    lat, lon, city = get_user_location()
    soil_data = fetch_soil(lat, lon)
    weather_data = fetch_weather(lat, lon)

# ------------------- Enhanced Sidebar -------------------
with st.sidebar:
    st.header("üéõÔ∏è ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£ ‡§™‡•à‡§®‡§≤")
    
    # Settings
    st.subheader("‚öôÔ∏è ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏")
    st.session_state.voice_enabled = st.checkbox("üîä ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§™‡•ç‡§≤‡•á‡§¨‡•à‡§ï", value=st.session_state.voice_enabled)
    st.session_state.auto_play_response = st.checkbox("üéµ ‡§∏‡•ç‡§µ‡§ö‡§æ‡§≤‡§ø‡§§ ‡§™‡•ç‡§≤‡•á‡§¨‡•à‡§ï", value=st.session_state.auto_play_response)
    
    response_length = st.selectbox(
        "‡§â‡§§‡•ç‡§§‡§∞ ‡§ï‡•Ä ‡§≤‡§Ç‡§¨‡§æ‡§à",
        ["‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§", "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø", "‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§"],
        index=1
    )
    
    # Reset button with confirmation
    if st.button("‚ôªÔ∏è ‡§ö‡•à‡§ü ‡§∞‡•Ä‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç", type="secondary"):
        if st.session_state.chat_history:
            st.session_state.chat_history = []
            st.success("‡§ö‡•à‡§ü ‡§∞‡•Ä‡§∏‡•á‡§ü ‡§π‡•ã ‡§ó‡§à!")
            time.sleep(1)
            st.rerun()
        else:
            st.info("‡§ï‡•ã‡§à ‡§ö‡•à‡§ü ‡§π‡§ø‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à")

    # Environmental data display
    st.header("üìä ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§°‡•á‡§ü‡§æ")
    st.success(f"üìç ‡§∏‡•ç‡§•‡§æ‡§®: {city}")

    with st.expander("üå± ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§ï‡•Ä ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.metric("pH ‡§∏‡•ç‡§§‡§∞", f"{soil_data.get('ph', 0):.1f}", 
                     delta=f"{soil_data.get('ph', 0) - 7.0:.1f} ‡§∏‡•á ‡§®‡•ç‡§Ø‡•Ç‡§ü‡•ç‡§∞‡§≤")
            st.metric("‡§∞‡•á‡§§ %", f"{soil_data.get('sand', 0):.0f}")
            st.metric("‡§ó‡§æ‡§¶ %", f"{soil_data.get('silt', 0):.0f}")
        with col2:
            st.metric("‡§®‡§æ‡§á‡§ü‡•ç‡§∞‡•ã‡§ú‡§®", f"{soil_data.get('nitrogen', 0):.0f}")
            st.metric("‡§ï‡§æ‡§∞‡•ç‡§¨‡§® %", f"{soil_data.get('organic_carbon', 0):.1f}")
            st.metric("‡§ö‡§ø‡§ï‡§®‡•Ä ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä %", f"{soil_data.get('clay', 0):.0f}")

    with st.expander("üå§Ô∏è ‡§Æ‡•å‡§∏‡§Æ ‡§ï‡•Ä ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.metric("‡§§‡§æ‡§™‡§Æ‡§æ‡§®", f"{weather_data.get('temperature', 0):.1f}¬∞C")
            st.metric("‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§π‡•ã‡§§‡§æ ‡§π‡•à", f"{weather_data.get('feels_like', weather_data.get('temperature', 25)):.1f}¬∞C")
            st.metric("‡§Ü‡§∞‡•ç‡§¶‡•ç‡§∞‡§§‡§æ", f"{weather_data.get('humidity', 0):.0f}%")
        with col2:
            st.metric("‡§¨‡§æ‡§∞‡§ø‡§∂", f"{weather_data.get('precipitation', 0):.1f}mm")
            st.metric("‡§π‡§µ‡§æ ‡§ï‡•Ä ‡§ó‡§§‡§ø", f"{weather_data.get('wind_speed', 0):.1f}km/h")
            if "uv" in weather_data:
                st.metric("UV ‡§∏‡•Ç‡§ö‡§ï‡§æ‡§Ç‡§ï", f"{weather_data.get('uv', 0):.0f}")
        
        st.info(f"‡§Æ‡•å‡§∏‡§Æ: {weather_data.get('condition', '‡§∏‡§æ‡§´‡§º')}")

    # Crop prediction
    predicted_crop, confidence = get_crop_prediction(soil_data, weather_data)
    st.success(f"üéØ ‡§∏‡•Å‡§ù‡§æ‡§à ‡§ó‡§à ‡§´‡§∏‡§≤: {predicted_crop}")
    
    # Enhanced confidence display
    confidence_color = "green" if confidence > 80 else "orange" if confidence > 60 else "red"
    st.markdown(f"‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§∏‡•ç‡§§‡§∞: :{confidence_color}[{confidence:.1f}%]")

# ------------------- Enhanced Groq LLM setup -------------------
try:
    MODEL_NAME = "llama-3.3-70b-versatile"
    llm = ChatGroq(
        groq_api_key=GROQ_API_KEY,
        model_name=MODEL_NAME,
        temperature=0.7,
        streaming=True,
        max_tokens=1024
    )

    # Enhanced prompt template with context
    template_text = """
‡§Ü‡§™ ‡§è‡§ï ‡§Ö‡§®‡•Å‡§≠‡§µ‡•Ä ‡§î‡§∞ ‡§¶‡•ã‡§∏‡•ç‡§§‡§æ‡§®‡§æ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§Æ‡§ø‡§§‡•ç‡§∞ ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§ï‡•É‡§∑‡§ø ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞ ‡§ï‡§æ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ 
Agar koi aap se puche apko kisne banaya, to kaho "AgroMind team ne, jo aapke kisan bhaiyon ke liye best AI assistant banane mein laga hai".

‡§Ü‡§™‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Ç:
- ‡§π‡§Æ‡•á‡§∂‡§æ ‡§∏‡§∞‡§≤, ‡§∏‡§Æ‡§ù‡§®‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡§æ
- ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç (‡§Æ‡•å‡§∏‡§Æ, ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä) ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§®‡§æ  
- "‡§≠‡§æ‡§à", "‡§ú‡•Ä", "‡§Ü‡§á‡§è" ‡§ú‡•à‡§∏‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡§æ‡§®‡§æ ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡§æ
- ‡§õ‡•ã‡§ü‡•á, actionable steps ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡§æ

‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§°‡•á‡§ü‡§æ:
- ‡§∏‡•ç‡§•‡§æ‡§®: {location}
- ‡§§‡§æ‡§™‡§Æ‡§æ‡§®: {temperature}¬∞C, ‡§Ü‡§∞‡•ç‡§¶‡•ç‡§∞‡§§‡§æ: {humidity}%
- ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä pH: {soil_ph}, ‡§®‡§æ‡§á‡§ü‡•ç‡§∞‡•ã‡§ú‡§®: {nitrogen}
- AI ‡§∏‡•Å‡§ù‡§æ‡§µ: {crop_suggestion} (‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏: {confidence:.1f}%)

‡§®‡§ø‡§Ø‡§Æ:
1. ‡§Ø‡§¶‡§ø ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á‡§ü ‡§∞‡•á‡§ü/‡§Æ‡§Ç‡§°‡•Ä ‡§≠‡§æ‡§µ ‡§™‡•Ç‡§õ‡•á‡§Ç ‡§§‡•ã ‡§ï‡§π‡•á‡§Ç: "‡§Ø‡§π ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§Ö‡§≠‡•Ä ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§π‡•à, ‡§ú‡§≤‡•ç‡§¶ ‡§π‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã‡§ó‡•Ä"
2. ‡§´‡§∏‡§≤ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ä‡§™‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§°‡•á‡§ü‡§æ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
3. ‡§π‡§Æ‡•á‡§∂‡§æ ‡§™‡•ç‡§∞‡•à‡§ï‡•ç‡§ü‡§ø‡§ï‡§≤ ‡§î‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§®‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§Ç
4. ‡§Ö‡§ó‡§∞ ‡§ï‡•ã‡§à ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§∏‡§≤‡§æ‡§π ‡§™‡•Ç‡§õ‡•á ‡§§‡•ã ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§®‡•á ‡§ï‡•ã ‡§ï‡§π‡•á‡§Ç
Aur jis salwal ka jawab aapko nahi pata, usme aap seedha "‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•á‡§¶ ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§á‡§∏ ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§∏‡§ï‡§§‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§" keh dena.

‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§∏‡§µ‡§æ‡§≤: {question}
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", template_text),
        ("user", "{question}")
    ])
    chain = prompt | llm | StrOutputParser()
    
except Exception as e:
    st.error(f"‚ùå LLM ‡§Æ‡•â‡§°‡§≤ ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ: {e}")
    st.info("‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡•Ä ‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§î‡§∞ GROQ_API_KEY ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡•á‡§Ç")
    st.stop()
# ------------------- LLM Response Function -------------------
def get_llm_response(user_question: str) -> str:
    """Generate LLM response"""
    if not user_question.strip():
        return "‡§ï‡•É‡§™‡§Ø‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§≤‡§ø‡§ñ‡•á‡§Ç‡•§"
    
    try:
        full_response = ""
        for chunk in chain.stream({
            "question": user_question,
            "location": city,
            "temperature": weather_data['temperature'],
            "humidity": weather_data['humidity'],
            "soil_ph": soil_data['ph'],
            "nitrogen": soil_data['nitrogen'],
            "crop_suggestion": predicted_crop,
            "confidence": confidence
        }):
            full_response += chunk
        
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": full_response,
            "timestamp": datetime.now().isoformat()
        })
        
        return full_response
    except Exception as e:
        logger.error(f"LLM error: {e}")
        return "‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§™‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§"




# ------------------- Streamlit UI -------------------
st.markdown('<div class="voice-section">', unsafe_allow_html=True)
st.subheader("üé§ ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§∏‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡•á‡§Ç")

col1, col2, col3 = st.columns([1, 2, 1])
with col2:
      wav_audio_data = st_audiorec()
      st.caption("‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§¨‡§ü‡§® ‡§¶‡•ã ‡§¨‡§æ‡§∞ ‡§î‡§∞ ‡§∏‡•ç‡§ü‡•â‡§™ ‡§¨‡§ü‡§® ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§¶‡§¨‡§æ‡§è‡§Ç")

    
      if wav_audio_data is not None:
        if wav_audio_data != st.session_state.last_audio_data:
            st.session_state.last_audio_data = wav_audio_data
            st.audio(wav_audio_data, format="audio/wav")
            st.success("üéµ ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§π‡•ã ‡§ó‡§Ø‡§æ!")
        
        if st.button("üîé ‡§ú‡§µ‡§æ‡§¨ ‡§™‡§æ‡§è‡§Ç", type="primary", disabled=st.session_state.processing):
            st.session_state.processing = True
            temp_path = None
            
            try:
                # Save temp file
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                    tmp.write(wav_audio_data)
                    temp_path = tmp.name
                
                # Transcribe
                with st.spinner("üîÑ ‡§∏‡§Æ‡§ù ‡§∞‡§π‡•á ‡§π‡•à‡§Ç..."):
                    voice_text = st.session_state.stt.transcribe(temp_path)
                
                if not voice_text:
                    st.error("‚ùå ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§à")
                else:
                    st.success(f"üó£Ô∏è {voice_text}")
                    
                    # Get response
                    with st.spinner("ü§ñ ‡§ú‡§µ‡§æ‡§¨ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç..."):
                        response = get_llm_response(voice_text)
                    
                    st.markdown(f"### ü§ñ {response}")
                    
                    # Save to history
                    st.session_state.chat_history.append({
                        "role": "user",
                        "content": voice_text,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    # Generate audio - NO THREADING, direct call
                    if st.session_state.voice_enabled and response:
                        with st.spinner("üéß ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç..."):
                            audio_bytes = st.session_state.tts_system.generate_audio(response)
                            
                            if audio_bytes:
                                st.audio(audio_bytes, format="audio/mp3")
                                st.success("üîä ‡§§‡•à‡§Ø‡§æ‡§∞!")
                            else:
                                st.info("üí° ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§™‡§¢‡§º‡•á‡§Ç")
            
            except Exception as e:
                st.error(f"‚ùå ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {str(e)}")
                logger.error(f"Voice processing error: {e}")
            finally:
                if temp_path:
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
                st.session_state.processing = False
st.markdown('</div>', unsafe_allow_html=True)

# ------------------- Enhanced Chat History Display -------------------
st.subheader("üí¨ ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡§æ ‡§á‡§§‡§ø‡§π‡§æ‡§∏")

if st.session_state.chat_history:
    for i, message in enumerate(st.session_state.chat_history):
        role = message.get("role")
        content = message.get("content", "")
        msg_type = message.get("type", "text")
        timestamp = message.get("timestamp", "")

        if role == "user":
            st.markdown(f'<div class="user-message">', unsafe_allow_html=True)
            icon = "üé§" if msg_type == "voice" else "‚úçÔ∏è"
            st.markdown(f"**{icon} ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ:** {content}")
            if timestamp:
                st.caption(f"‚è∞ {timestamp[:19].replace('T', ' ')}")
            st.markdown('</div>', unsafe_allow_html=True)

        else:  # assistant message
            st.markdown(f'<div class="assistant-message">', unsafe_allow_html=True)
            st.markdown(f"**ü§ñ AI ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞:** {content}")
            if timestamp:
                st.caption(f"‚è∞ {timestamp[:19].replace('T', ' ')}")

            # Audio playback button for assistant messages
            if st.session_state.voice_enabled:
                col1, col2 = st.columns([1, 4])
                with col1:
                    if st.button(f"üîä", key=f"play_audio_{i}", help="‡§á‡§∏ ‡§ú‡§µ‡§æ‡§¨ ‡§ï‡•ã ‡§∏‡•Å‡§®‡•á‡§Ç"):
                        with st.spinner("üéß ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç..."):
                            audio_bytes = st.session_state.tts_system.generate_audio(content)
                            if audio_bytes:
                                st.audio(audio_bytes, format="audio/mp3")

            st.markdown('</div>', unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)

else:
   st.markdown("""
<style>
.chat-container {
    background-color: #000000;  
    color: #FFFFFF;           
    padding: 20px;
    border-radius: 15px;
    box-shadow: 0px 4px 10px rgba(0,0,0,0.5);
    margin-top: 20px;
    margin-bottom: 20px;
    font-family: 'Segoe UI', sans-serif;
}
.chat-container h4 {
    font-size: 1.8rem;
    margin-bottom: 10px;
    color: #00FF7F;
}
.chat-container ul li {
    margin-bottom: 5px;
}
.chat-container em {
    color: #FFD700;
}
</style>

<div class="chat-container">
    <h4>üëã ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ï‡§ø‡§∏‡§æ‡§® ‡§≠‡§æ‡§à!</h4>
    <p>‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡§æ AI ‡§ï‡•É‡§∑‡§ø ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞ ‡§π‡•Ç‡§Ç‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§µ‡§ø‡§∑‡§Ø‡•ã‡§Ç ‡§™‡§∞ ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç:</p>
    <ul>
        <li>üåæ <strong>‡§´‡§∏‡§≤ ‡§ï‡•Ä ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂</strong> - ‡§ï‡•å‡§® ‡§∏‡•Ä ‡§´‡§∏‡§≤ ‡§¨‡•ã‡§è‡§Ç</li>
        <li>üå± <strong>‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§ï‡•Ä ‡§¶‡•á‡§ñ‡§≠‡§æ‡§≤</strong> - ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡•á ‡§§‡§∞‡•Ä‡§ï‡•á</li>
        <li>üåßÔ∏è <strong>‡§Æ‡•å‡§∏‡§Æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§∏‡§≤‡§æ‡§π</strong> - ‡§Æ‡•å‡§∏‡§Æ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§ñ‡•á‡§§‡•Ä</li>
        <li>üêõ <strong>‡§ï‡•Ä‡§ü ‡§î‡§∞ ‡§∞‡•ã‡§ó ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£</strong> - ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§®</li>
        <li>üíß <strong>‡§∏‡§ø‡§Ç‡§ö‡§æ‡§à ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§®</strong> - ‡§™‡§æ‡§®‡•Ä ‡§ï‡•Ä ‡§∏‡§π‡•Ä ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ</li>
        <li>üåø <strong>‡§ú‡•à‡§µ‡§ø‡§ï ‡§ñ‡•á‡§§‡•Ä</strong> - ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§§‡§∞‡•Ä‡§ï‡•á</li>
    </ul>
    <p><em>‡§Ü‡§™ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§≤‡§ø‡§ñ‡§ï‡§∞ ‡§Ø‡§æ ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç!</em></p>
</div>
""", unsafe_allow_html=True)


# ------------------- Enhanced Text Input Section -------------------
def process_text_input(user_input: str):
    """Process text input using unified LLM function"""
    if st.session_state.processing:
        st.warning("‚è≥ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§è‡§ï ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à...")
        return
    
    st.session_state.processing = True
    try:
        # Display user message
        with st.chat_message("user"):
            st.markdown(f"‚úçÔ∏è {user_input}")
        
        # Save to history
   
        # LLM response
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            response_placeholder.markdown("ü§ñ ‡§∏‡•ã‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç... üß†")
            
            full_response = ""
            try:
                for chunk in chain.stream({
                    "question": user_input,
                    "location": city,
                    "temperature": weather_data.get('temperature', 25),
                    "humidity": weather_data.get('humidity', 70),
                    "soil_ph": soil_data.get('ph', 6.5),
                    "nitrogen": soil_data.get('nitrogen', 50),
                    "crop_suggestion": predicted_crop,
                    "confidence": confidence
                }):
                    full_response += chunk
                    response_placeholder.markdown(f"ü§ñ {full_response}")
                    
            except Exception as e:
                error_msg = f"‡§ú‡§µ‡§æ‡§¨ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ: {str(e)}"
                response_placeholder.error(f"‚ùå {error_msg}")
                full_response = "‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§∏‡§ï‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§"
                logger.error(f"LLM generation error: {e}")
        
        st.session_state.chat_history.append({
            "role": "user", 
            "content": user_input, 
            "type": "text",
            "timestamp": datetime.now().isoformat()
        })
    
        # Generate audio - NO THREADING, direct call
        if st.session_state.voice_enabled and full_response:
            with st.spinner("üéß ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç..."):
                audio_bytes = st.session_state.tts_system.generate_audio(full_response)

                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3")
                    st.success("üîä ‡§§‡•à‡§Ø‡§æ‡§∞!")
                else:
                    st.info("üí° ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ú‡§µ‡§æ‡§¨ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§®‡§æ‡§à ‡§ú‡§æ ‡§∏‡§ï‡•Ä‡•§")

    except Exception as e:
        st.error(f"‚ùå ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ: {str(e)}")
        logger.error(f"Text processing error: {e}")
    finally:
        st.session_state.processing = False

# Handle chat input
if user_input := st.chat_input("‚úçÔ∏è ‡§Ö‡§™‡§®‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§Ø‡§π‡§æ‡§Å ‡§≤‡§ø‡§ñ‡•á‡§Ç..."):
    process_text_input(user_input)

# ------------------- Enhanced Footer Section -------------------
st.markdown("---")

# Statistics and utilities
col1, col2, col3, col4 = st.columns(4)

with col1:
    total_chats = len([m for m in st.session_state.chat_history if m["role"] == "user"])
    st.metric("üí¨ ‡§ï‡•Å‡§≤ ‡§∏‡§µ‡§æ‡§≤", total_chats)

with col2:
    voice_chats = len([m for m in st.session_state.chat_history if m.get("type") == "voice"])
    st.metric("üé§ ‡§Ü‡§µ‡§æ‡§ú‡§º‡•Ä ‡§∏‡§µ‡§æ‡§≤", voice_chats)

with col3:
    if st.session_state.chat_history:
        last_chat_time = st.session_state.chat_history[-1].get("timestamp", "")
        if last_chat_time:
            st.metric("‚è∞ ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§∏‡§µ‡§æ‡§≤", last_chat_time[:19].replace('T', ' '))
    else:
        st.metric("‚è∞ ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§∏‡§µ‡§æ‡§≤", "‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç")

with col4:
    # Export chat functionality
    if st.button("üì• ‡§ö‡•à‡§ü ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§ï‡§∞‡•á‡§Ç"):
        if st.session_state.chat_history:
            chat_export_data = {
                "export_timestamp": datetime.now().isoformat(),
                "location_info": {
                    "city": city,
                    "coordinates": [lat, lon],
                    "weather": weather_data,
                    "soil": soil_data
                },
                "crop_prediction": {
                    "recommended_crop": predicted_crop,
                    "confidence": confidence
                },
                "chat_history": st.session_state.chat_history,
                "statistics": {
                    "total_messages": len(st.session_state.chat_history),
                    "voice_messages": voice_chats,
                    "text_messages": total_chats - voice_chats
                }
            }
            
            # Create downloadable JSON
            json_str = json.dumps(chat_export_data, ensure_ascii=False, indent=2)
            
            st.download_button(
                label="üíæ JSON ‡§´‡§º‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç",
                data=json_str,
                file_name=f"agriculture_chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                help="‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡•ã JSON ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡•á‡§µ ‡§ï‡§∞‡•á‡§Ç"
            )
        else:
            st.info("‡§ï‡•ã‡§à ‡§ö‡•à‡§ü ‡§π‡§ø‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à")

# Quick action buttons
st.markdown("### üöÄ ‡§§‡•ç‡§µ‡§∞‡§ø‡§§ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®")
col1, col2, col3 = st.columns(3)

quick_questions = [
    "‡§á‡§∏ ‡§Æ‡•å‡§∏‡§Æ ‡§Æ‡•á‡§Ç ‡§ï‡•å‡§® ‡§∏‡•Ä ‡§´‡§∏‡§≤ ‡§¨‡•á‡§π‡§§‡§∞ ‡§π‡•ã‡§ó‡•Ä?",
    "‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§ï‡•Ä ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§ï‡•à‡§∏‡•á ‡§∏‡•Å‡§ß‡§æ‡§∞‡•á‡§Ç?",
    "‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è?"
]

for i, (col, question) in enumerate(zip([col1, col2, col3], quick_questions)):
    with col:
        if st.button(question, key=f"quick_q_{i}"):
            process_text_input(question)

# Help and information section
with st.expander("‚ÑπÔ∏è ‡§Æ‡§¶‡§¶ ‡§î‡§∞ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä", expanded=False):
    st.markdown("""
    ### üîß ‡§ï‡•à‡§∏‡•á ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§∞‡•á‡§Ç:
    
    **‡§Ü‡§µ‡§æ‡§ú‡§º ‡§∏‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è:**
    1. üé§ "‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§°" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç
    2. ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§¨‡•ã‡§≤‡•á‡§Ç
    3. "‡§∏‡•ç‡§ü‡•â‡§™" ‡§¶‡§¨‡§æ‡§ï‡§∞ ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§°‡§ø‡§Ç‡§ó ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•á‡§Ç  
    4. "‡§ú‡§µ‡§æ‡§¨ ‡§™‡§æ‡§è‡§Ç" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç
    
    **‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§∏‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è:**
    1. ‡§®‡•Ä‡§ö‡•á ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§≤‡§ø‡§ñ‡•á‡§Ç
    2. Enter ‡§¶‡§¨‡§æ‡§è‡§Ç ‡§Ø‡§æ ‡§≠‡•á‡§ú‡•á‡§Ç ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç
    
    ### üåæ ‡§Æ‡•à‡§Ç ‡§ï‡§ø‡§® ‡§µ‡§ø‡§∑‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç:
    - ‡§´‡§∏‡§≤ ‡§ö‡•Å‡§®‡§®‡•á ‡§ï‡•Ä ‡§∏‡§≤‡§æ‡§π (‡§Æ‡•å‡§∏‡§Æ ‡§î‡§∞ ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞)
    - ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡•á ‡§§‡§∞‡•Ä‡§ï‡•á
    - ‡§ï‡•Ä‡§ü ‡§î‡§∞ ‡§¨‡•Ä‡§Æ‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§∞‡•ã‡§ï‡§•‡§æ‡§Æ
    - ‡§∏‡§ø‡§Ç‡§ö‡§æ‡§à ‡§ï‡§æ ‡§∏‡§π‡•Ä ‡§∏‡§Æ‡§Ø ‡§î‡§∞ ‡§§‡§∞‡•Ä‡§ï‡§æ
    - ‡§ñ‡§æ‡§¶ ‡§î‡§∞ ‡§â‡§∞‡•ç‡§µ‡§∞‡§ï ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä
    - ‡§ú‡•à‡§µ‡§ø‡§ï ‡§ñ‡•á‡§§‡•Ä ‡§ï‡•á ‡§®‡•Å‡§∏‡•ç‡§ñ‡•á
    - ‡§Æ‡•å‡§∏‡§Æ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§ñ‡•á‡§§‡•Ä ‡§ï‡•Ä ‡§Ø‡•ã‡§ú‡§®‡§æ
    
    ### ‚ö†Ô∏è ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∏‡•Ç‡§ö‡§®‡§æ:
    - ‡§Ø‡§π ‡§è‡§ï AI ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•à ‡§î‡§∞ ‡§¶‡•Ä ‡§ó‡§à ‡§∏‡§≠‡•Ä ‡§∏‡§≤‡§æ‡§π ‡§ï‡•á‡§µ‡§≤ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§π‡•à‡§Ç
    - ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ï‡•É‡§∑‡§ø ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§ï‡•É‡§∑‡§ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§∏‡•á ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§≤‡•á‡§Ç
    - ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á‡§ü ‡§∞‡•á‡§ü ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Ö‡§≠‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à (‡§ú‡§≤‡•ç‡§¶ ‡§Ü‡§è‡§ó‡•Ä)
    
    ### üõ†Ô∏è ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ:
    - ‡§Ø‡§¶‡§ø ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§™‡§π‡§ö‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§π‡•ã ‡§§‡•ã ‡§∂‡§æ‡§Ç‡§§ ‡§ú‡§ó‡§π ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡•á‡§Ç
    - ‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§ß‡•Ä‡§Æ‡§æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§•‡•ã‡§°‡§º‡§æ ‡§á‡§Ç‡§§‡§ú‡§º‡§æ‡§∞ ‡§ï‡§∞‡•á‡§Ç
    - ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è "‡§ö‡•à‡§ü ‡§∞‡•Ä‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç" ‡§¨‡§ü‡§® ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
    """)

# Footer with credits and version info
st.markdown("""
<div style='text-align: center; color: #666; margin-top: 2rem; padding: 1rem; border-top: 1px solid #ddd;'>
    <p>üåæ <strong>AI ‡§ï‡•É‡§∑‡§ø ‡§∏‡§π‡§æ‡§Ø‡§ï(By AgroMind)</strong> - ‡§Ü‡§™‡§ï‡•á ‡§ñ‡•á‡§§ ‡§ï‡§æ ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§Æ‡§ø‡§§‡•ç‡§∞</p>
    <p><small>‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ 2.0 | Powered by Groq AI , SoilGrids & OpenWeatherMap</small></p>
    <p><small>
        ‡§∏‡§≠‡•Ä ‡§∏‡§≤‡§æ‡§π ‡§ï‡•á‡§µ‡§≤ ‡§∏‡•Ç‡§ö‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•à‡§Ç‡•§ 
        ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ï‡•É‡§∑‡§ø ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§ï‡•É‡§∑‡§ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§∏‡•á ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§Ö‡§µ‡§∂‡•ç‡§Ø ‡§≤‡•á‡§Ç‡•§
    </small></p>
</div>
""", unsafe_allow_html=True)
