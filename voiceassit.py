import os
import tempfile
import streamlit as st
from st_audiorec import st_audiorec
import requests
from gtts import gTTS
import io
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import with error handling
try:
    from voice_pipeline import *
except ImportError:
    st.error("‚ùå voice_pipeline module not found!")
    get_llm_response = None
    chain = None

# --- üîë Keys ‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç (Streamlit Cloud Compatible) ---
try:
    GROQ_API_KEY = st.secrets.get("GROQ_API_KEY", "").strip()
except (FileNotFoundError, KeyError):
    GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()


@st.cache_data(ttl=300)
def get_supported_languages():
    """‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§≠‡§æ‡§∑‡§æ‡§è‡§Ç (cached)"""
    return {
        "hi": "üáÆüá≥ ‡§π‡§ø‡§Ç‡§¶‡•Ä",
        "en": "üá¨üáß English",
        "mr": "üáÆüá≥ ‡§Æ‡§∞‡§æ‡§†‡•Ä",
        "gu": "üáÆüá≥ ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä",
        "ta": "üáÆüá≥ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç",
        "te": "üáÆüá≥ ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å",
        "bn": "üáÆüá≥ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ",
        "pa": "üáÆüá≥ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä"
    }


def cleanup_temp_files(*file_paths):
    """‡§Ö‡§∏‡•ç‡§•‡§æ‡§à ‡§´‡§æ‡§á‡§≤‡•á‡§Ç ‡§∏‡§æ‡§´ ‡§ï‡§∞‡•á‡§Ç"""
    for fp in file_paths:
        try:
            if fp and os.path.exists(fp):
                os.unlink(fp)
        except Exception:
            pass


def transcribe_audio(audio_path: str, language: str = "hi") -> str:
    """
    Groq Whisper API ‡§∏‡•á ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§ï‡•ã ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡•á‡§Ç (Optimized)
    
    Args:
        audio_path: ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ ‡§ï‡§æ path
        language: ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•ã‡§° (default: hi)
    
    Returns:
        ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü
    """
    if not GROQ_API_KEY:
        raise ValueError("GROQ_API_KEY ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä!")
    
    url = "https://api.groq.com/openai/v1/audio/transcriptions"
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}"}
    
    with open(audio_path, "rb") as f:
        files = {"file": (os.path.basename(audio_path), f, "audio/wav")}
        data = {
            "model": "whisper-large-v3",  # More accurate than turbo
            "language": language,
            "response_format": "verbose_json",  # Get confidence scores
            "temperature": 0.0,
            "prompt": get_language_prompt(language)  # Context hint for better accuracy
        }
        resp = requests.post(url, headers=headers, data=data, files=files, timeout=90)
    
    resp.raise_for_status()
    result = resp.json()
    
    # Return text with confidence check
    text = result.get("text", "").strip()
    
    # Log confidence if available (for debugging)
    if "segments" in result and result["segments"]:
        avg_confidence = sum(s.get("no_speech_prob", 0) for s in result["segments"]) / len(result["segments"])
        if avg_confidence > 0.8:
            st.warning("‚ö†Ô∏è Audio quality low. Speak clearly near microphone.")
    
    return text


def get_language_prompt(lang: str) -> str:
    """Language-specific prompts for better transcription"""
    prompts = {
        "hi": "‡§Ø‡§π ‡§è‡§ï ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§µ‡•â‡§Ø‡§∏ ‡§®‡•ã‡§ü ‡§π‡•à‡•§ ‡§∏‡§π‡•Ä ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§",
        "en": "This is a clear English voice note with proper pronunciation.",
        "mr": "‡§π‡•á ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§≠‡§æ‡§∑‡•á‡§§‡•Ä‡§≤ ‡§Ü‡§µ‡§æ‡§ú ‡§∏‡§Ç‡§¶‡•á‡§∂ ‡§Ü‡§π‡•á‡•§",
        "gu": "‡™Ü ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä ‡™≠‡™æ‡™∑‡™æ‡™Æ‡™æ‡™Ç ‡™µ‡´â‡™á‡™∏ ‡™®‡´ã‡™Ç‡™ß ‡™õ‡´á.",
        "ta": "‡Æá‡Æ§‡ØÅ ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æ© ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æï‡ØÅ‡Æ∞‡Æ≤‡Øç ‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡ØÅ.",
        "te": "‡∞á‡∞¶‡∞ø ‡∞∏‡±ç‡∞™‡∞∑‡±ç‡∞ü‡∞Æ‡±à‡∞® ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞µ‡∞æ‡∞Ø‡∞ø‡∞∏‡±ç ‡∞®‡±ã‡∞ü‡±ç.",
        "bn": "‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶Ø‡¶º‡ßá‡¶∏ ‡¶®‡ßã‡¶ü‡•§",
        "pa": "‡®á‡®π ‡®á‡©±‡®ï ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä ‡®µ‡©å‡®á‡®∏ ‡®®‡©ã‡®ü ‡®π‡©à‡•§"
    }
    return prompts.get(lang, "")


def text_to_speech(text: str, lang: str = "hi") -> bytes:
    """
    ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡•ã ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡•á‡§Ç (gTTS) - Returns bytes instead of file
    
    Args:
        text: ‡§¨‡•ã‡§≤‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü
        lang: ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•ã‡§°
    
    Returns:
        ‡§ë‡§°‡§ø‡§Ø‡•ã bytes
    """
    # Limit text length for TTS
    max_chars = 3000
    if len(text) > max_chars:
        text = text[:max_chars] + "..."
    
    # Use faster speech rate
    tts = gTTS(text=text, lang=lang, slow=False)
    
    # Save to BytesIO instead of file (faster)
    audio_bytes = io.BytesIO()
    tts.write_to_fp(audio_bytes)
    audio_bytes.seek(0)
    
    return audio_bytes.getvalue()


def initialize_session_state():
    """Session state ‡§ï‡•ã initialize ‡§ï‡§∞‡•á‡§Ç"""
    defaults = {
        "audio_path": None,
        "transcript": "",
        "ai_response": "",
        "processing": False,
        "conversation_history": [],
        "chat_history": [],
        "audio_quality_tips_shown": False,
        "voice_enabled": True,
        "tts_system": None
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def show_audio_quality_tips():
    """Audio quality tips"""
    if not st.session_state.audio_quality_tips_shown:
        with st.expander("üí° ‡§¨‡•á‡§π‡§§‡§∞ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ü‡§ø‡§™‡•ç‡§∏", expanded=True):
            st.markdown("""
            **‡§∏‡§π‡•Ä ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§¨‡•ã‡§≤‡•á‡§Ç:**
            - üì± ‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§´‡•ã‡§® ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§∞‡§π‡•á‡§Ç (15-20 cm)
            - üîá ‡§∂‡§æ‡§Ç‡§§ ‡§ú‡§ó‡§π ‡§ö‡•Å‡§®‡•á‡§Ç
            - üó£Ô∏è ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§î‡§∞ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡•á‡§Ç
            - ‚è∏Ô∏è ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§õ‡•ã‡§ü‡§æ pause ‡§¶‡•á‡§Ç
            - üé§ ‡§Ö‡§ö‡•ç‡§õ‡•Ä audio quality ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•á‡§°‡§´‡•ã‡§® ‡§Æ‡§æ‡§á‡§ï use ‡§ï‡§∞‡•á‡§Ç
            """)
            st.session_state.audio_quality_tips_shown = True


def voice_assistant_feature():
    """
    üéô Optimized Streamlit Cloud Voice Assistant
    - Fast processing
    - Better speech recognition
    - Improved error handling
    """
    
    # Initialize session state
    initialize_session_state()
    
    # API key check
    if not GROQ_API_KEY:
        st.error("‚ùå **GROQ_API_KEY ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä!**")
        st.info("""
        **Streamlit Cloud ‡§™‡§∞ setup:**
        1. App settings ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§è‡§Ç
        2. Secrets ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§è‡§Ç
        3. Add ‡§ï‡§∞‡•á‡§Ç:
        ```toml
        GROQ_API_KEY = "your_key_here"
        ```
        """)
        return
    
    # Check if voice_pipeline is available
    if 'chain' not in globals() or chain is None:
        st.error("‚ùå voice_pipeline module ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à!")
        return
    
    # Header with tips
    col_head1, col_head2 = st.columns([3, 1])
    with col_head1:
        st.markdown("#### üéôÔ∏è ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡•á‡§Ç")
    with col_head2:
        if st.button("üí° Tips"):
            st.session_state.audio_quality_tips_shown = False
    
    show_audio_quality_tips()
    
    # Language selector
    languages = get_supported_languages()
    col1, col2 = st.columns([3, 1])
    
    with col2:
        selected_lang = st.selectbox(
            "‡§≠‡§æ‡§∑‡§æ ‡§ö‡•Å‡§®‡•á‡§Ç",
            options=list(languages.keys()),
            format_func=lambda x: languages[x],
            key="language_selector",
            label_visibility="collapsed"
        )
    
    with col1:
        # Recording tips inline
        st.caption("üî¥ Record ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç ‚Üí ‡§¨‡•ã‡§≤‡•á‡§Ç ‚Üí Stop ‡§¶‡§¨‡§æ‡§è‡§Ç")
    
    audio_bytes = st_audiorec()
    
    if audio_bytes:
        # Check audio size
        audio_size_kb = len(audio_bytes) / 1024
        if audio_size_kb < 5:
            st.warning("‚ö†Ô∏è Audio ‡§¨‡§π‡•Å‡§§ ‡§õ‡•ã‡§ü‡•Ä ‡§π‡•à‡•§ 2-3 ‡§∏‡•á‡§ï‡§Ç‡§° ‡§§‡§ï ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§")
        elif audio_size_kb > 1024:
            st.warning("‚ö†Ô∏è Audio ‡§¨‡§π‡•Å‡§§ ‡§¨‡§°‡§º‡•Ä ‡§π‡•à‡•§ ‡§ï‡§Æ ‡§¨‡•ã‡§≤‡•á‡§Ç ‡§Ø‡§æ ‡§¶‡•ã‡§¨‡§æ‡§∞‡§æ ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡•á‡§Ç‡•§")
        
        # Save to temporary file
        if st.session_state.audio_path:
            cleanup_temp_files(st.session_state.audio_path)
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tfile:
            tfile.write(audio_bytes)
            st.session_state.audio_path = tfile.name
        
        # Process button
        col_btn1, col_btn2, col_btn3 = st.columns([2, 1, 1])
        
        with col_btn1:
            process_btn = st.button(
                "üöÄ ‡§™‡•Ç‡§õ‡•á‡§Ç AI ‡§∏‡•á",
                type="primary",
                use_container_width=True,
                disabled=st.session_state.processing
            )
        
        with col_btn2:
            if st.button("üîÑ Re-record", use_container_width=True):
                cleanup_temp_files(st.session_state.audio_path)
                st.session_state.audio_path = None
                st.rerun()
        
        with col_btn3:
            if st.button("üóëÔ∏è Clear", use_container_width=True):
                cleanup_temp_files(st.session_state.audio_path)
                st.session_state.audio_path = None
                st.session_state.transcript = ""
                st.session_state.ai_response = ""
                st.rerun()
        
        if process_btn:
            st.session_state.processing = True
            transcript = ""
            
            # Progress tracking
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Transcription
            status_text.text("üß† ‡§Ü‡§™‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§ï‡•ã ‡§∏‡§Æ‡§ù ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... (1/3)")
            progress_bar.progress(33)
            
            try:
                transcript = transcribe_audio(st.session_state.audio_path, selected_lang)
                st.session_state.transcript = transcript
                
                if not transcript:
                    st.error("‚ùå ‡§ï‡•ã‡§à ‡§∂‡§¨‡•ç‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§")
                    st.session_state.processing = False
                    progress_bar.empty()
                    status_text.empty()
                    return
                    
            except requests.exceptions.Timeout:
                st.error("‚ùå Timeout: Network slow ‡§π‡•à‡•§ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§")
                st.session_state.processing = False
                progress_bar.empty()
                status_text.empty()
                return
            except requests.exceptions.RequestException as e:
                st.error(f"‚ùå API Error: {str(e)}")
                st.info("üí° Check your internet connection ‡§î‡§∞ GROQ API key")
                st.session_state.processing = False
                progress_bar.empty()
                status_text.empty()
                return
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                st.session_state.processing = False
                progress_bar.empty()
                status_text.empty()
                return
            
            # Step 2: Get AI Response
            if transcript:
                st.success(f"‚úÖ **‡§Ü‡§™‡§®‡•á ‡§ï‡§π‡§æ:** {transcript}")
                
                status_text.text("üí¨ AI ‡§ú‡§µ‡§æ‡§¨ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à... (2/3)")
                progress_bar.progress(66)
                
                full_response = ""
                try:
                    response_placeholder = st.empty()
                    response_placeholder.markdown("ü§ñ ‡§∏‡•ã‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç... üß†")
                    
                    # Stream response from chain
                    for chunk in chain.stream({"question": transcript}):
                        full_response += chunk
                        response_placeholder.markdown(f"**ü§ñ AI ‡§ï‡§æ ‡§ú‡§µ‡§æ‡§¨:**\n\n{full_response}")
                    
                    st.session_state.ai_response = full_response
                        
                except Exception as e:
                    error_msg = f"‡§ú‡§µ‡§æ‡§¨ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ: {str(e)}"
                    response_placeholder.error(f"‚ùå {error_msg}")
                    full_response = "‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§∏‡§ï‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§"
                    logger.error(f"LLM generation error: {e}")
                
                # Save to chat history
                st.session_state.chat_history.append({
                    "role": "user", 
                    "content": transcript, 
                    "type": "text",
                    "timestamp": datetime.now().isoformat()
                })
                
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": full_response, 
                    "type": "text",
                    "timestamp": datetime.now().isoformat()
                })
                
                # Save to conversation history
                st.session_state.conversation_history.append({
                    "user": transcript,
                    "ai": full_response,
                    "lang": selected_lang
                })
                
                # Step 3: Generate audio response
                if full_response and full_response != "‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§∏‡§ï‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§":
                    status_text.text("üéô ‡§ú‡§µ‡§æ‡§¨ ‡§ï‡•ã ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... (3/3)")
                    progress_bar.progress(90)
                    
                    try:
                        # Check if TTS system is available from voice_pipeline
                        if st.session_state.voice_enabled and hasattr(st.session_state, 'tts_system') and st.session_state.tts_system:
                            audio_bytes_tts = st.session_state.tts_system.generate_audio(full_response)
                        else:
                            # Fallback to gTTS
                            audio_bytes_tts = text_to_speech(full_response, selected_lang)
                        
                        if audio_bytes_tts:
                            progress_bar.progress(100)
                            status_text.text("‚úÖ ‡§™‡•Ç‡§∞‡§æ ‡§π‡•Å‡§Ü!")
                            st.audio(audio_bytes_tts, format="audio/mp3")
                        else:
                            st.info("üí° ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ú‡§µ‡§æ‡§¨ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§®‡§æ‡§à ‡§ú‡§æ ‡§∏‡§ï‡•Ä‡•§")
                            
                    except Exception as e:
                        st.error(f"‚ùå TTS Error: {str(e)}")
                        st.info("üí° ‡§Ü‡§™ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ú‡§µ‡§æ‡§¨ ‡§ä‡§™‡§∞ ‡§™‡§¢‡§º ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§")
                
                # Clear progress indicators
                progress_bar.empty()
                status_text.empty()
            
            st.session_state.processing = False
            
            # Auto-scroll to response
            st.markdown('<div id="response"></div>', unsafe_allow_html=True)
    
    else:
        st.info("üëÜ ‡§ä‡§™‡§∞ üî¥ Record ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç ‡§î‡§∞ ‡§¨‡•ã‡§≤‡•á‡§Ç...")
    
    # Show conversation history (last 5)
    if st.session_state.conversation_history:
        st.markdown("---")
        with st.expander("üìú ‡§™‡§ø‡§õ‡§≤‡•Ä ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§", expanded=False):
            for i, conv in enumerate(reversed(st.session_state.conversation_history[-5:])):
                col_hist1, col_hist2 = st.columns([1, 20])
                with col_hist1:
                    st.markdown(f"**{len(st.session_state.conversation_history)-i}.**")
                with col_hist2:
                    st.markdown(f"**üë§:** {conv['user']}")
                    with st.expander("ü§ñ AI Response"):
                        st.markdown(conv['ai'])
                if i < min(4, len(st.session_state.conversation_history) - 1):
                    st.divider()
    
    # Footer
    st.markdown("---")
    col_f1, col_f2, col_f3 = st.columns([2, 1, 1])
    
    with col_f1:
        st.caption("‚ú® Powered by Whisper-v3 + GPT + gTTS")
    
    with col_f2:
        st.caption(f"üí¨ {len(st.session_state.conversation_history)} conversations")
    
    with col_f3:
        if st.button("üóë Clear All"):
            st.session_state.conversation_history = []
            st.session_state.chat_history = []
            cleanup_temp_files(st.session_state.audio_path)
            st.session_state.audio_path = None
            st.success("‚úÖ Cleared!")
            st.rerun()


# Cleanup on session end
def cleanup_on_session_end():
    """Session end ‡§™‡§∞ cleanup"""
    try:
        if hasattr(st.session_state, 'audio_path'):
            cleanup_temp_files(st.session_state.audio_path)
    except Exception:
        pass
