import os
import io
import time
import tempfile
import requests
import streamlit as st
import logging
from typing import Optional
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv
from gtts import gTTS

# Langchain / Groq imports
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ------------------- Logging Setup -------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ------------------- Load Environment Variables -------------------
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

# Validation
if not GROQ_API_KEY:
    st.error("‚ùå .env ‡§´‡§º‡§æ‡§á‡§≤ ‡§Æ‡•á‡§Ç `GROQ_API_KEY` ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç")
    st.info("üí° Groq API key: https://console.groq.com")
    st.stop()

# Initialize OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ------------------- Unified TTS System -------------------
class UnifiedTTSSystem:
    """Unified TTS with OpenAI primary and gTTS fallback (Cloud-safe)"""
    
    def __init__(self):
        self.openai_available = openai_client is not None
        self.cache = {}
        
    def generate_audio(self, text: str, use_cache: bool = True) -> Optional[bytes]:
        """Generate audio from text with caching & fallback"""
        if not text or not text.strip():
            return None
        
        text = text.strip()
        if len(text) > 500:
            text = self._truncate_intelligently(text, 500)
        
        # üîÅ Cache check
        text_hash = hash(text[:500])
        if use_cache:
            cached = self.cache.get(text_hash)
            if cached:
                logger.info("‚úÖ Using cached TTS audio")
                return cached
        
        # üîä Try OpenAI TTS
        audio_bytes = None
        if self.openai_available:
            audio_bytes = self._openai_tts(text)
        
        # üîÅ Fallback to gTTS if OpenAI fails
        if audio_bytes is None:
            logger.warning("‚ö†Ô∏è OpenAI TTS failed, switching to gTTS")
            audio_bytes = self._gtts_tts(text)
        
        # üß† Store in cache
        if use_cache and audio_bytes and len(self.cache) < 20:
            self.cache[text_hash] = audio_bytes
        
        return audio_bytes
    
    def _openai_tts(self, text: str) -> Optional[bytes]:
        """OpenAI TTS implementation (cloud-ready)"""
        try:
            response = openai_client.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input=text[:4096],
            )
            return response.read()
        except Exception as e:
            logger.error(f"üõë OpenAI TTS error: {e}")
            return None
    
    def _gtts_tts(self, text: str) -> Optional[bytes]:
        """gTTS fallback (stream-safe)"""
        try:
            if len(text) > 4000:
                text = text[:4000]  # Prevent long audio timeout on cloud
            tts = gTTS(text=text, lang="hi", slow=False)
            audio_buffer = io.BytesIO()
            tts.write_to_fp(audio_buffer)
            audio_buffer.seek(0)
            return audio_buffer.read()
        except Exception as e:
            logger.error(f"üõë gTTS error: {e}")
            return None
    
    def _truncate_intelligently(self, text: str, max_length: int) -> str:
        """Truncate intelligently at sentence boundaries"""
        if len(text) <= max_length:
            return text
        
        sentences = text.split('‡•§')
        truncated = ""
        for sentence in sentences:
            if len(truncated + sentence + "‡•§") <= max_length:
                truncated += sentence + "‡•§"
            else:
                break
        
        return truncated if truncated else text[:max_length] + "..."

# ------------------- Speech-to-Text -------------------
class SpeechToText:
    """Speech-to-Text using Groq Whisper API (Cloud-Safe)"""
    
    @staticmethod
    def transcribe(audio_bytes: bytes, filename: str = "audio.wav", language: str = "hi") -> str:
        """
        Transcribe audio from in-memory bytes (works with Streamlit Cloud)
        - audio_bytes: raw audio data from upload or recording
        - filename: helps API detect file type
        """
        try:
            if not audio_bytes or len(audio_bytes) < 1000:
                logger.error("‚ö†Ô∏è No valid audio data received.")
                return ""
            
            # Create a file-like object
            audio_file = io.BytesIO(audio_bytes)
            audio_file.name = filename  # üî• Required for Groq Whisper
            
            # Use OpenAI client (works with Groq too)
            client = OpenAI(api_key=GROQ_API_KEY, base_url="https://api.groq.com/openai/v1")
            transcript = client.audio.transcriptions.create(
                model="whisper-large-v3",
                file=audio_file,
                language=language,
                response_format="text"
            )
            
            return transcript.strip()
        
        except Exception as e:
            logger.error(f"üõë Transcription error: {e}")
            return ""



# ------------------- Session State -------------------
def init_session_state():
    """Initialize session state"""
    if "initialized" not in st.session_state:
        st.session_state.update({
            "initialized": True,
            "chat_history": [],
            "processing": False,
            "last_audio": None,
            "tts_system": UnifiedTTSSystem(),
            "stt": SpeechToText(),
            # Mock data for demo
            "city": "‡§á‡§Ç‡§¶‡•å‡§∞",
            "weather_data": {"temperature": 28, "humidity": 65},
            "soil_data": {"ph": 6.5, "nitrogen": 45},
            "predicted_crop": "‡§ó‡•á‡§π‡•Ç‡§Ç",
            "confidence": 85.5
        })

init_session_state()


# ------------------- LLM Setup -------------------
try:
    MODEL_NAME = "llama-3.3-70b-versatile"
    llm = ChatGroq(
        groq_api_key=GROQ_API_KEY,
        model_name=MODEL_NAME,
        temperature=0.7,
        streaming=True,
        max_tokens=1024
    )

    # Enhanced prompt template with context
    template_text = """
‡§Ü‡§™ ‡§è‡§ï ‡§Ö‡§®‡•Å‡§≠‡§µ‡•Ä ‡§î‡§∞ ‡§¶‡•ã‡§∏‡•ç‡§§‡§æ‡§®‡§æ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§Æ‡§ø‡§§‡•ç‡§∞ ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§ï‡•É‡§∑‡§ø ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞ ‡§ï‡§æ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ 
Agar koi aap se puche apko kisne banaya, to kaho "AgroMind team ne, jo aapke kisan bhaiyon ke liye best AI assistant banane mein laga hai".

‡§Ü‡§™‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Ç:
- ‡§π‡§Æ‡•á‡§∂‡§æ ‡§∏‡§∞‡§≤, ‡§∏‡§Æ‡§ù‡§®‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡§æ
- ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç (‡§Æ‡•å‡§∏‡§Æ, ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä) ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§®‡§æ  
- "‡§≠‡§æ‡§à", "‡§ú‡•Ä", "‡§Ü‡§á‡§è" ‡§ú‡•à‡§∏‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡§æ‡§®‡§æ ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡§æ
- ‡§õ‡•ã‡§ü‡•á, actionable steps ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡§æ

‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§°‡•á‡§ü‡§æ:
- ‡§∏‡•ç‡§•‡§æ‡§®: {location}
- ‡§§‡§æ‡§™‡§Æ‡§æ‡§®: {temperature}¬∞C, ‡§Ü‡§∞‡•ç‡§¶‡•ç‡§∞‡§§‡§æ: {humidity}%
- ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä pH: {soil_ph}, ‡§®‡§æ‡§á‡§ü‡•ç‡§∞‡•ã‡§ú‡§®: {nitrogen}
- AI ‡§∏‡•Å‡§ù‡§æ‡§µ: {crop_suggestion} (‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏: {confidence:.1f}%)

‡§®‡§ø‡§Ø‡§Æ:
1. ‡§Ø‡§¶‡§ø ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á‡§ü ‡§∞‡•á‡§ü/‡§Æ‡§Ç‡§°‡•Ä ‡§≠‡§æ‡§µ ‡§™‡•Ç‡§õ‡•á‡§Ç ‡§§‡•ã ‡§ï‡§π‡•á‡§Ç: "‡§Ø‡§π ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§Ö‡§≠‡•Ä ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§Æ‡•á‡§Ç ‡§π‡•à, ‡§ú‡§≤‡•ç‡§¶ ‡§π‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã‡§ó‡•Ä"
2. ‡§´‡§∏‡§≤ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ä‡§™‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§°‡•á‡§ü‡§æ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
3. ‡§π‡§Æ‡•á‡§∂‡§æ ‡§™‡•ç‡§∞‡•à‡§ï‡•ç‡§ü‡§ø‡§ï‡§≤ ‡§î‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§®‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§Ç
4. ‡§Ö‡§ó‡§∞ ‡§ï‡•ã‡§à ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§∏‡§≤‡§æ‡§π ‡§™‡•Ç‡§õ‡•á ‡§§‡•ã ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§®‡•á ‡§ï‡•ã ‡§ï‡§π‡•á‡§Ç
Aur jis salwal ka jawab aapko nahi pata, usme aap seedha "‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•á‡§¶ ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§á‡§∏ ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§∏‡§ï‡§§‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§" keh dena.

‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§∏‡§µ‡§æ‡§≤: {question}
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", template_text),
        ("user", "{question}")
    ])
    chain = prompt | llm | StrOutputParser()
    
except Exception as e:
    st.error(f"‚ùå LLM ‡§Æ‡•â‡§°‡§≤ ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ: {e}")
    st.info("‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡•Ä ‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§î‡§∞ GROQ_API_KEY ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡•á‡§Ç")
    st.stop()
# ------------------- LLM Response Function -------------------
def get_llm_response(user_question: str) -> str:
    """Generate LLM response"""
    if not user_question.strip():
        return "‡§ï‡•É‡§™‡§Ø‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§≤‡§ø‡§ñ‡•á‡§Ç‡•§"
    
    try:
        full_response = ""
        response_placeholder = st.empty()
        
        for chunk in chain.stream({
            "question": user_question,
            "location": st.session_state.city,
            "temperature": st.session_state.weather_data['temperature'],
            "humidity": st.session_state.weather_data['humidity'],
            "soil_ph": st.session_state.soil_data['ph'],
            "nitrogen": st.session_state.soil_data['nitrogen'],
            "crop_suggestion": st.session_state.predicted_crop,
            "confidence": st.session_state.confidence
        }):
            full_response += chunk
            response_placeholder.markdown(f"**ü§ñ ‡§ú‡§µ‡§æ‡§¨:** {full_response}‚ñå")
        
        response_placeholder.markdown(f"**ü§ñ ‡§ú‡§µ‡§æ‡§¨:** {full_response}")
        
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": full_response,
            "timestamp": datetime.now().isoformat()
        })
        
        return full_response
        
    except Exception as e:
        logger.error(f"LLM error: {e}")
        return "‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§™‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§"

# ------------------- Voice Input Section -------------------
